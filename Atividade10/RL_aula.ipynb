{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aprendizado por reforço ou Reinforcement Learning(RL) utilizando o método Q-Learning com o ambiente simulado Gym\n",
    "\n",
    "Q-Learning é um algoritmo de aprendizado por reforço que visa aprender a função de valor Q(s,a) onde \"s\" é um estado e \"a\" é uma ação. O objetivo do Q-Learning é encontrar a política ótima que maximiza a recompensa acumulada esperada.\n",
    "\n",
    "Gym, desenvolvido pela OpenAI, é uma biblioteca que fornece ambientes de simulação para experimentação com algoritmos de aprendizado por reforço. Ele fornece uma interface padronizada para uma variedade de ambientes, facilitando o desenvolvimento e teste de algoritmos de RL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Será realizado o problema do Táxi, onde o táxi precisa aprender a levar passageiros de um local (R, G, B, Y) para o outro\n",
    "\n",
    "* As letras em azul é o local de origem e as letras rosa é o local de destino\n",
    "* O táxi fica amarelo quando está vazio e verde quando está com um passageiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random.seed(1234)\n",
    "\n",
    "streets = gym.make(\"Taxi-v3\", render_mode='ansi').env \n",
    "streets.reset()\n",
    "print(\"\\n\" + streets.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* O táxi pode andar nas 4 direções cardeais onde estiver \":\" e não pode andar onde estiver \"|\"\n",
    "* O algoritimo Q-Learning dará ao táxi 20 pontos quando ele alcançar sucesso, descontará 10 pontos caso pegue ou deixe o passageiro em uma possição ilegal e descontará 1 ponto por campo andado com o passgeiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "initial_state = streets.encode(2,3,2,0)\n",
    "streets.s = initial_state\n",
    "print(\"\\n\" + streets.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 368, -1, False)],\n",
       " 1: [(1.0, 168, -1, False)],\n",
       " 2: [(1.0, 288, -1, False)],\n",
       " 3: [(1.0, 248, -1, False)],\n",
       " 4: [(1.0, 268, -10, False)],\n",
       " 5: [(1.0, 268, -10, False)]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streets.P[initial_state]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada linha corresponde a uma possivel ação:\n",
    "* Mover em uma das 4 direções;\n",
    "* Pegar ou deixar o passageiro.\n",
    "\n",
    "Os 4 valores nas linhas indicam:\n",
    "* A probabilidade de acatar a ação;\n",
    "* O próximo estado da ação;\n",
    "* A recompensa da ação;\n",
    "* Se a ação é um local bem sucedido para deixar o passageiro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hora de treinar nosso Táxi!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialmente, a tabela q_table é preenchida com zeros, indicando que o valor esperado de qualquer par estado-ação é desconhecido, conforme o agente interage com o ambiente, esses valores são atualizados com base nas recompensas recebidas e nas estimativas das recompensas futuras.\n",
    "Durante o treinamento, o agente usa a tabela q_table para escolher ações baseadas na política derivada dos valores Q(s,a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "número de diferentes situações ou posições que o agente pode encontrar no ambiente:  500\n",
      "número de escolhas que o agente pode fazer em qualquer estado:  6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q_table = np.zeros([streets.observation_space.n, streets.action_space.n]) \n",
    "\n",
    "print(\"número de diferentes situações ou posições que o agente pode encontrar no ambiente: \", streets.observation_space.n)\n",
    "print(\"número de escolhas que o agente pode fazer em qualquer estado: \", streets.action_space.n)\n",
    "print()\n",
    "\n",
    "learning_rate = 0.1 # velocidade de treinamento\n",
    "discount_factor = 0.6 # determina o quanto o algoritmo valoriza as recompensas a longo prazo em comparação com as recompensas imediatas\n",
    "exploration = 0.1 # 10% de chance de fazer uma escolha aleatoria ao inves de sequir o valor Q\n",
    "epochs = 10000 # irá acontecer 10.000 simulações\n",
    "\n",
    "for run in range(epochs):\n",
    "    state = streets.reset()[0] # para pegar apenas o estado atual\n",
    "    # print(state)\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        # print(q_table)\n",
    "        random_value = random.uniform(0,1)\n",
    "        if(random_value < exploration):\n",
    "            action = streets.action_space.sample() # explora qualquer ação aleatória\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # escolhe a ação com o maior valor Q\n",
    "        \n",
    "        # print(streets.step(action))    \n",
    "        next_state, reward, done, parameter, info = streets.step(action)\n",
    "        \n",
    "        prev_q = q_table[state, action]\n",
    "        next_max_q = np.max(q_table[next_state])\n",
    "        new_q = (1 - learning_rate) * prev_q + learning_rate * (reward + discount_factor * next_max_q) # formula do valor Q\n",
    "        q_table[state, action] = new_q\n",
    "\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.42292084, -2.40486172, -2.40083173, -2.3639511 , -7.30794855,\n",
       "       -6.36084921])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table[initial_state] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trip number: 4\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "for tripnum in range(1,5):\n",
    "    state = streets.reset()[0]\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        next_state, reward, done, parameter, info = streets.step(action)\n",
    "        clear_output(wait=True)\n",
    "        print(\"Trip number: \" + str(tripnum))\n",
    "        print(streets.render())\n",
    "        sleep(.5)\n",
    "        state = next_state\n",
    "    sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
